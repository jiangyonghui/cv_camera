% !TeX root = ../thesis.tex

\chapter{Related Work}
\label{sec:related_work}

This chapter reviews some state-of-the-art works in the field of human detection and tracking, human action recognition as well as temporal action detection.

\section{Person Detection}
\label{related_work:pd}
The goal of object detection is to detect and localize a class of object in image or video. Person detection, as a specialized case of object detection, is a crucial pre-processing step for
many applications in computer vision, e.g. visual surveillance, autonomous vehicles and automated
video analysis, which is exactly the first step in our pipeline (Section \ref{pipeline:PD}). 

Early works \cite{Mohan:2001:EOD:377051.377062, viola2003} detect pedestrians using Haar-like features which have been successfully applied in face detection \cite{viola2001}. The Haar-like features are rectangular features similar to Haar basis functions which have been used by Papageorgiou et al. \cite{papag1998}; These simple features can be computed very rapidly using an the integral image. Viola et al. in \cite{viola2003} trained a human detector using an Adaboost algorithm, which selects a small number of classifiers iteratively from a large set of possible weak classifiers. The selected weak classifiers are combined into an effective classifier for detection. Mohan et al. \cite{Mohan:2001:EOD:377051.377062} present a detection framework that uses four example-based detectors to localize the head, legs, and arms on human body. Although these methods are very efficient, the Haar-like features are not capable of handling the complexity in real-world images and videos.

Background subtraction \cite{Elgammal2000, Siden2003, stau1999, toyama1999} is also an efficient way to detect moving objects. It extracts the foreground regions by calculating the difference between a video frame and the background model. However, it has a few limitations: it requires the video is taken by a static camera - only moving objects will be detected; it is difficult to separate humans in a group; and since it does not have a specific appearance model for the target, it cannot differentiate humans from other moving objects, for example cars or animals.

In 2005, Dalal and Triggs \cite{Dalal2005} proposed a Histogram of Oriental Gradients descriptor
(HOG) for object detection. In their method, a set of overlapping HOG features are extracted from a sliding window and then fed into an SVM \cite{Cortes1995}. The advantage of HOG features is that each
image cell is statistically represented by a histogram of the gradient orientations and magnitudes,
thus it is more invariant to illumination, shadows, etc. Wang and Han \cite{wang2009} proposed a HOG based detection framework that combines HOG features and Local Binary Pattern (LBP) \cite{ahonen2006} feature, which captures the subtle intensity changes in a small local area. Due to its superior performance and computational efficiency, LBP has become a popular feature for texture classification, face recognition, and object detection. Based on video, additional information such as optical flow or motion constraints can greatly assist the detection task. Dalal et al. \cite{dala2006} proposed a new histogram of optical flow feature and combined it with HOG feature. However, the computational cost of calculating motion features is relatively high.

With the success of deep learning methods in recent years \cite{krizhevsky2012}, detectors based on CNNs have been shown to outperform those traditional approaches. A seminal work in this area is R-CNN proposed by Girshick et al. \cite{DBLP:journals/corr/GirshickDDM13}, which makes use of a classification network by applying it to a set of pre-computed object proposals and feeding the extracted features to class-specific SVMs in order to classify each region independently. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.75\linewidth]{RelatedWork/r_cnn.png}
  \caption{\textbf{R-CNN object detection system overview.} (1) takes an input image, (2) extracts
           around 2000 bottom-up region proposals, (3) computes features for each proposal using a
           large convolutional neural network (CNN), and then (4) classifies each region using class-
           specific linear SVMs. \cite{DBLP:journals/corr/GirshickDDM13}}
  \label{fig:r_cnn}
\end{figure}

Its successors Fast R-CNN \cite{DBLP:journals/corr/Girshick15} and Faster R-CNN \cite{DBLP:journals/corr/RenHG015} further improve over it by sharing computation on single images with region of interest (RoI) pooling and integrating the region proposal generation into the network, respectively. 

Another two popular CNN based detectors are here to mention: Single Shot Multibox Detector (SSD) \cite{DBLP:journals/corr/LiuAESR15} and You Only Look Once (YOLO) \cite{DBLP:journals/corr/RedmonDGF15}, which operate in a fully-convolutional manner and do not rely on region proposals to generate detection hypotheses. The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class
instances in those boxes, followed by a non-maximum suppression step to produce the final detections \cite{DBLP:journals/corr/LiuAESR15}. Figure \ref{fig:ssd} shows how the SSD framework detects objects in the scene and Figure \ref{fig:yolo} presents how YOLO model works. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.65\linewidth]{RelatedWork/ssd.png}
  \caption{\textbf{SSD framework.} (a) SSD only needs an input image and ground truth boxes for each
           object during training. In a convolutional fashion, a small set (e.g. 4) of default boxes of
           different aspect ratios at each location in several feature maps with different scales is to 
           be evaluated (e.g. $8 \times 8$ and $4 \times 4$ in (b) and (c)). For each default box, both
           the shape offsets and the confidences for all object categories (($c_{1}, c_{2}, \cdots,
           c_{p}$)) are predicted. \cite{DBLP:journals/corr/LiuAESR15}}
  \label{fig:ssd}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\linewidth]{RelatedWork/yolo.png}
  \caption{\textbf{The YOLO model.} The system models detection as a regression problem. It divides the
           image into an $S \times S$ grid and for each grid cell predicts $B$ boundingboxes, 
           confidence for those boxes, and $C$ class probabilities. These predictions are encoded as an
           $S \times S \times (B * 5 + C)$ tensors. \cite{DBLP:journals/corr/RedmonDGF15}}
  \label{fig:yolo}
\end{figure}

\section{Person Tracking}
Person tracking is the task of tracking a person over several frames of an image sequence. It can be divided into two categories: Single Object Tracking (SOT) and Multiple Object Tracking (MOT). 

\subsection{Single Object Tracking}
Early works \cite{broida1986, shi1994} focus on estimate the motion parameters of the target. Shi et al. \cite{shi1994} uses the affine motion model to compensate the image motion and selects discriminative feature points for the tracker. Comaniciu et al. \cite{coman2003} proposed the mean-shift algorithm to track objects by finding the peak in a confidence map of the surrounding area. This confidence map is a probability density function calculated on each pixel using color similarity. The above-mentioned works provide basic tracking frameworks. However, the object models in these methods are too simple to handle real world videos.

Later discriminative tracking approaches with online-learning have been extensively explored. Avidan \cite{avidan2007} train an ensemble of weak classifiers using AdaBoost to distinguish between the object and the background. Collins and Liu \cite{collins2005} selects the best features to
track based on the two-class variance ratio between foreground and background. Grabner et al. \cite{Grabner2006} learns a discriminative classifier in an online manner to detect the object from the background. Babenko et al. \cite{Babenko2009} use Multiple Instance Learning (MIL) to avoid the drift caused by inaccurate bounding boxes.

In such methods, an object-specific detector is trained in a semi-supervised fashion and then used to locate the target in consecutive frames. In particular, a few positive and negative examples are collected each time the target is located, and these examples are immediately used to re-train the detector. In this way, the detector can capture the most discriminative features on the object. However, the online learned detector will often drift into the background in long-term tracking.

In recent years, a few sophisticated methods for single-target tracking have been proposed to address complex real world data. Kala \cite{kalal2010} leverages the video structure to select the most
confusing negative examples from backgrounds to strengthen the classifier. Dinh et al. \cite{dinh2011} tracks multiple objects beside the target, therefore it avoids the confusions with other objects with similar appearances. Hare et al. \cite{hare2016} present a framework for adaptive visual object tracking based on structured support vector machine, which provides superior tracking performance.
Another well-known tracking method raised by R.E.Kalman in 1960 is the Kalman filter \cite{Welch1995}, which is mostly used to solve the discrete-data linear filtering problem and has been applied in various research works like \cite{vasuhi2015} and \cite{Katsarakis2006}.

With sophisticated models and online-learning approaches, these methods are more invariant to appearance change and occlusion. However, their computational cost is relatively high for a practical tracking system. Moreover, in multi-person scenarios, SOT is not able to track multiple agents in real world video surveillance. 

\subsection{Multiple Object Tracking}
A large number of works have focused on multi-target tracking-by-detection algorithms. These methods tackle multi-target tracking by optimizing detection assignments over a temporal window, given certain global constraints. Blackman \cite{blackman2004} proposed a multiple hypothesis tracking framework: possible trajectory hypotheses are proposed and propagated into the future in anticipation that subsequent data will solve the difficult data association decisions. Zhang et al. \cite{zhang2008} resolve the association between the detection and the tracking by optimizing a cost-flow network with a non-overlap constraint on trajectories. Brendel et al. \cite{brendel2011} apply a maximum-weight independent set algorithm to merge small tracklets into long tracks. Benfold et al. \cite{benfold2011} use Markov-Chain Monte-Carlo Data Association (MCMCDA) to correspond the detections obtained by a HOG-based head detector in crowded scenes in multi-threads. Such methods employ offline-trained detectors to find the targets and associate them with the tracks. Another approach is developed by C.Kuo \cite{Kuo2010} for online learning of discriminative appearance models for robust multi-target tracking in a crowded scene. Within a time sliding window from tracklets, training samples are assembled online and also for combining effective descriptors and their similarity measurements, the AdaBoost algorithm is used.


\section{Action Recognition}
According the input modalities used for human action recognition, related works can be categorized into non-pose-based (Section \ref{subsec:non_pose_based_ar}) and pose-based (Section \ref{subsec:pose_based_ar}) approaches. The non-pose-based action recognition, which have been intensively studied over the past decades, employ features extracted from RGB image, depth map or optical flows. Pose-based action recognition, which is also the focus of this thesis, takes advantage of the high-level skeleton information.

\subsection{Non-Pose-based Action Recognition}
\label{subsec:non_pose_based_ar}
For solving human action recognition problem, researchers presented some approaches that are based on RGB frames and optical flows. They implemented the techniques to capture context information from RGB frames and motion information (e.g. optical flow). Some researchers presented their work in domain of conventional machine learning approach, i.e, extraction of handcrafted features from videos, encoding them with bag-of-words \cite{Zhang2010bow} and then training a classifier, such as SVM \cite{Cortes1995}, with these extracted descriptors to perform action classification. Apart from conventional machine learning approaches, many research works have also utilized deep learning for action classification. In the following, both handcrafted-feature-based methods and deep-learning-based methods will be discussed. 

\subsubsection{Handcrafted-Feature-based Approaches}
Before deep learning came out to dominate in the field of computer vision, the traditional pattern recognition problems rely on hand-crafted feature extractors, so does the task of action recognition. Most of approaches usually follow the classical pipeline of pattern recognition to first extract low-level appearance features like image gradients, or motion features such as optical flows and feature descriptors such as HOG \cite{Dalal2005}, HOF \cite{Laptev2008} or SIFT \cite{Lowe2004}, SURF \cite{Bay2008}. Then mid-level features are extracted through sparse dictionary learning or bag of words. Finally, the features are trained with a classifier or regressor in a supervised manner. 

\textbf{Improved Dense Trajectories} 

The work of improved dense trajectories \cite{wang2013} is acknowledged as the handcrafted feature-based approach owing to its great performance on several action recognition benchmarks. \\

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{RelatedWork/idt.png}
  \caption{Left: Dense sampling of feature points in different spatial scales. Middle: Tracking in an
           optical flow field. Right: descriptors along the trajectory. \cite{wang2013}}
  \label{fig:idt}
\end{figure}

As is shown in Figure \ref{fig:idt}, feature points at different spatial scales are densely sampled in the image. Points in homogeneous areas where no visual structure is available are removed since it is impossible to track any point on these areas. The second step is the tracking of the feature points along the temporal dimension in the dense optical flow field, in which there is a displacement vector at every single position that implies where the point has moved in the next frame. Along each trajectory, HOG, HOF and MBH (motion boundary histograms) \cite{dala2006} descriptors are extracted in the local coordinate system centered around the keypoint. At last, the action classification is performed in a standard pattern recognition pipeline: pooling of bag-of-features representation (generation of codebooks in an unsupervised manner) and then training the descriptors with an SVM classifier.

This work is an improvement upon \cite{wang2011} from the same author. Based on the former work, it compensates camera motion by matching feature points between frames using SURF descriptors and dense optical flow and estimating a homography with RANSAC \cite{Fischler1981}. On the challenging action datasets UCF50 \cite{Reddy2013} and HMDB51 \cite{kuehne2011}, this approach of improved dense trajectory \cite{wang2013} has achieved the state-of-the-art performance among all the non-deep-learning-based methods. 


\subsubsection{Deep-Learning-based Approaches}
With the advanced performance of convolutional neural networks (CNN) in image classification \cite{DBLP:journals/corr/SimonyanZ14a}, researchers utilized CNN for video recognition tasks. Unlike single static image classification, both the spatial and temporal pattern should be encoded inside the 3D volume in the scenario of video analysis. 

\textbf{Two Stream Convolutional Neural Networks} 

Simonyan et al. \cite{DBLP:journals/corr/SimonyanZ14} proposed a two-stream ConvNet architecture, as is illustrated in Figure \ref{fig:two_stream_convnet}, which models the temporal information locally is to exploit handcrafted features that contain local motion information. The spatial stream operates on individual RGB frames while the temporal stream operates on stacking of optical flows of multiple frames. The softmax scores of both streams are fused lately at the very end of the architecture and it turns out that the fusion of two streams has better performance than either stream alone, which implies that temporal and spatial streams are complementary. Based on the same two-stream architecture, \cite{DBLP:journals/corr/FeichtenhoferPZ16} studies different approaches of fusing spatial and temporal CNNs. According to the experiment results, it is better to fuse the two networks at the last convolutional layer than earlier. It also proposes an additional fusion in the class prediction layer to further boost the accuracy. \\

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\linewidth]{RelatedWork/two_stream_convnet.png}
  \caption{Two-stream structure for video classification. \cite{DBLP:journals/corr/SimonyanZ14}}
  \label{fig:two_stream_convnet}
\end{figure}


\textbf{Temporal Segment Networks}

Another successful application of adapted two-stream network was the Temporal Segment Networks (TSN) \cite{DBLP:journals/corr/WangXW0LTG16}. TSN is also built upon the conventional two-stream architecture as shown in Figure \ref{fig:tsn}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{RelatedWork/tsn.png}
  \caption{Complete pipeline of Temporal Segment Networks. \cite{DBLP:journals/corr/WangXW0LTG16}}
  \label{fig:tsn}
\end{figure} 

However, unlike \cite{DBLP:journals/corr/SimonyanZ14a}, it has proposed a sparse and global temporal sampling strategy to extract short snippets over the entire video sequence. Each video sequence is divided into several equal-lengthed segments and a short snippet is randomly selected from each segment to remove redundant information in consecutive frames. Each snippet is represented by several modalities such as RGB images, stacked optical flow fields, stacked RGB differences and stacked warped optical flow fields. Then the class scores of all the snippets are aggregated with an segmental consensus function and softmax predictions of all the modalities are fused at the end to determine the final class scores. The segmental consensus is derived from all the snippet-level prediction. By back propagation with the loss function upon segmental consensus, the network is optimized using visual information from the entire video. TSN has also achieved the state-of-art performance on two challenging action recognition benchmarks HMDB51 \cite{kuehne2011} and UCF101 \cite{Soomro2012}.

Apart from TSN, recurrent neural network (RNN) is a more conventional way to encode long-term temporal. Donahue et al. \cite{DBLP:journals/corr/DonahueHGRVSD14} has leveraged CNNs for extraction of visual features and Long Short-Term Memory (LSTM) \cite{Hochreiter1997} for sequential learning of the features, leading to predictions of every single frame of a video. Similarly, the work from Google research \cite{DBLP:journals/corr/NgHVVMT15} uses CNNs for extraction of features from RGB images and optical flows. Then it employs an LSTM for feature aggregation. \\


\subsection{Pose-based Action Recognition}
\label{subsec:pose_based_ar}
A human body can be represented as an articulated system of rigid limbs connected by several body joints and human action can be regarded as a continuous temporal evolution of the spatial configuration of these body joints. A distinct gesture together with certain movement of 2D/3D body joints can highly predicts the intention of the human subject. Under this precondition, the main idea of pose-based action recognition is to track the movements and displacements of human body joints in every frame and then classify the temporal evolution of the human skeletons. \\

\textbf{2D Human Pose Estimation by Part Affinity Fields}

Cao et al. \cite{cao2017realtime} proposed an efficient technique to detect 2D pose of multiple
persons in the image. The technique comprises of two branch multi-stage CNNs. The architecture of this technique is shown in Figure \ref{fig:openpose_arch}. The branch shown in beige colour is trained to predict the confidence maps S of joint locations. The second branch shown in blue colour, is trained to predict the affinity fields L between the adjacent joints. The affinity fields is a 2D vector field, which points from one joint location to adjacent joint, as shown in Figure \ref{fig:openpose}(c). Each branch is trained with intermediate supervision at each stage, that ensures the refinement of predictions over successive stages. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{RelatedWork/openpose_arch.png}
  \caption{Multi stage two branch architecture for pose estimation by part affinity field. 
           \cite{cao2017realtime}}
  \label{fig:openpose_arch}
\end{figure} 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{RelatedWork/openpose.png}
  \caption{Overall pipeline of pose estimation by part affinity field technique. 
           \cite{cao2017realtime}}
  \label{fig:openpose}
\end{figure} 

During training, the image is first analysed by standard VGG-19 \cite{Vedaldi2015} convolutional neural network, which generates a set of feature maps $F$. In the first stage, these features maps $F$ serve as inputs to both branches. These branches produces a set of confidence maps predictions $S^{1}$ and a set of affinity fields predictions $L^{1}$ in the first stage. In the subsequent stages, both of these predictions $S^{1}$, $L^{1}$ along with feature maps $F$, are concatenated and used to produce refined predictions. The network is trained on large COCO dataset \cite{DBLP:journals/corr/LinMBHPRDZ14}. The trained network takes RGB images as inputs and predicts confidence maps of each joint along with part affinity fields as shown in Figure \ref{fig:openpose}(b),(c). The final pose of each person is found by part association algorithm. In this algorithm, the association between candidate part detection is measured by computing the line integral over the corresponding part affinity field, along the line segment connecting the joint locations. Our pipeline utilizes the OpenPose\footnote{\url{https://github.com/CMU-Perceptual-Computing-Lab/openpose}}, which is based on this work, as the 2D pose estimator. \\

\textbf{Monocular 3D Pose Estimation}

Another pose estimation framework is proposed in \cite{DBLP:journals/corr/ZhouH0XW17}, which is a weakly-supervised approach for 3D pose estimation from monocular RGB frames. The framework architecture is illustrated in Fig \ref{fig:3d_pose_model}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{RelatedWork/3d_pose_model.png}
  \caption{The stacked hourglass network computes the 2D heat maps (2D joints) based on the image
           features extracted by Conv layers and the depth regression module computes the depth value
           based on summation of 2D heat maps and image features extracted in the beginning.
           \cite{DBLP:journals/corr/ZhouH0XW17}}
  \label{fig:3d_pose_model}
\end{figure} 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\linewidth]{RelatedWork/3d_pose_arch.png}
  \caption{A schematic illustration of our method: transferring 3D annotation from indoor images to 
          in-the-wild images. Top (Training): Both indoor images with 3D annotation (Right) and 
          in-the-wild images with 2D annotation (Left) are used to train the deep neural network.
          Bottom (Testing): The learned network can predict the 3D pose of the human in in-the-wild
          images. \cite{DBLP:journals/corr/ZhouH0XW17}}
  \label{fig:3d_pose_arch}
\end{figure} 


The proposed network splits the task into estimation of 2D joints and regression of depth value based on the estimated 2D joints. The architecture consists of a 2D pose estimation module which is imported from the state-of-art hourglass architecture in \cite{DBLP:conf/eccv/NewellYD16} and a depth regression module. This architecture not only benefits from the existing sophisticated 2D pose estimation algorithms but also overcomes the domain difference between datasets in indoor laboratory setting where 3D joints are obtained by motion capture system and datasets in the wild that have annotations of 2D joints. As is shown in Figure \ref{fig:3d_pose_arch}, for 3D data, 3D Euclidean loss is applied for standard regression, while weakly-supervised loss based on 2D Euclidean loss and the sum of variance of bone length ratios in several body parts (the ratio between estimated bone length and the average bone length among all training data) is applied for 2D data. As the depth information is inferred from a monocular 2D image by data-hungry learning techniques instead of triangulation of stereo vision, the estimated pose can be referred to as “pseudo-3D” skeleton data.


\section{Temporal Action Detection}
\label{subsec:tem_action_det}
In the context of video-based human action recognition, temporal action detection and grouping is of vital importance, as videos in real applications are usually long, untrimmed and contain multiple action instances. The challenge for our pipeline lies not only in recognizing action categories based on human pose and other features, but also detecting the start time (frame) and end time (frame) of each action instance. Many state-of-the-art methods \cite{DBLP:journals/corr/Girshick15, DBLP:journals/corr/GirshickDDM13, DBLP:journals/corr/RenHG015} adopt the "detection by classifying region proposals" framework: first do proposal, and then classify proposals. The main drawback of this framework is that proposal generation and classification procedures are separate and have to be trained seperately. Lin et al. \cite{lin2017} proposed a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. As illustrated in Figure \ref{fig:ssad}, the SSAD network take the Snippet-level Action Score features as input and predicts the actions directly, which already contains the start and end information of the snippets. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.6\linewidth]{RelatedWork/ssad.png}
  \caption{\textbf{Overview of SSAD framework}. Given an untrimmed long video, (1) Snippet-level Action
           Score features sequence with multiple action classifiers are extracted; (2) SSAD network
           takes feature sequence as input and directly predicts multiple scales action instances
           without proposal generation step. \cite{lin2017}}
  \label{fig:ssad}
\end{figure} 
 
Another action grouping strategy proposed in \cite{DBLP:journals/corr/XiongZWLT17} can efficiently generate candidates with accurate temporal boundaries, while in the fashion of $proposal + classification$. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{RelatedWork/action_grouping.png}
  \caption{An illustration of the temporal actionness grouping algorithm. 
           \cite{DBLP:journals/corr/XiongZWLT17}}
  \label{fig:action_grouping}
\end{figure} 

As shown in Figure \ref{fig:action_grouping}, the scheme first obtains a number of action fragments by thresholding – a fragment here is a consecutive sub-sequence of snippets whose actionness scores are above a certain threshold $\tau$. Then, to generate a region proposal, a fragment is picked as a starting point and it will be expanded recursively by absorbing succeeding fragments. The expansion terminates when the portion of low-actionness snippets goes beyond $\gamma$, a positive value which is referred to as the tolerance threshold. Beginning with different fragments, a collection of different region proposals can be obtained \cite{DBLP:journals/corr/XiongZWLT17}.
