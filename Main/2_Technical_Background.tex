% !TeX root = ../thesis.tex

\chapter{Technical Background}
\label{sec:technical_background}

In order to conduct a better presentation of this work, some fundamental knowledge in human detection and tracking, action detection and recognition, deep learning as well as Robot Operating System will be covered in this chapter. 

\section{Cues for Action Recognition}
Action recognition can either be based on still images \cite{zhang_arimage_2016} or videos \cite{TSN2016ECCV}. However, still images doesn’t provide motion information that is required to predict motion-based actions. Therefore, only action recognition based on videos is discussed in this thesis. The videos provide a wide range of applications with varied actions. For these varied actions, either context information is relevant or motion information is vital or definite posture information is necessary. As RGB is meant to give context information, optical flows provide motion information and pose contributes in giving spatial posture of humans, therefore, the three cues (RGB, optical flow, pose) are essential for action recognition and they are presented here in detail.

\subsection{RGB}
Every video can be regarded as an arbitrarily-lengthed sequence of RGB frames. RGB frames (images) stack together to form a video as shown in Figure \ref{fig:rgb_cue}. An RGB image is a matrix with three channels where each channel has the corresponding intensity value ranging from 0 to 255 of the primary colors red, blue and green respectively. A blob of different pixels forms a structural representation of the context information contained in the scene. As convolutional neural networks have achieved great success in image-based tasks such as image classification and object detection \cite{zhao2018object}, it is comprehensible that CNN is competent in extracting effective features from image patterns.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\linewidth]{TechnicalBackground/RGB.png}
  \caption{RGB cue}
  \label{fig:rgb_cue}
\end{figure}

\subsection{Optical Flow}
Optical flow is a canonical vision-based cue for modeling of local motion in video sequences.
It describes the distribution of apparent movement or velocities of brightness patterns in
an image, which is caused by relative motion between the objects and the viewer \cite{Horn81determiningoptical}. Given two frames at time $t$ and time $t + \Delta t$ from the same video, an optical flow algorithm solves for a motion field where the displacement vector at every single position implies how the pixel has moved from the first frame to the second. 

We can derive the equation which relates the change in image brightness at a point to the
motion of the brightness pattern. We denote the image intensity by $I(x, y, t)$ as a function of the point $(x, y)$ in the image plane and time $t$. Under the assumption of brightness constancy, pixel intensities are translated from one frame to the next,
\begin{equation}
I(x, y, t) = I(x + \Delta x, y + \Delta y, t + \Delta t)
\end{equation}
According to the Taylor series of the right side of the equation, we have,
\begin{equation}
I(x + \Delta x, y + \Delta y, t + \Delta t) \approx I(x, y, t) + \frac{\partial I}{\partial x}\Delta x + \frac{\partial I}{\partial y}\Delta y + \frac{\partial I}{\partial t}\Delta t
\end{equation}
Then we acquire the gradient constraint equation for optical flow
\begin{equation}
\nabla I(\mathbf{x}, t)\cdot \mathbf{u} + I_{t}(\mathbf{x}, t) = 0
\end{equation}
where $\nabla I = (I_{x}, I_{y})$ denotes the image gradient functions along the x and y direction, as shown in Figure \ref{fig:opflow_cue_x} and Figure \ref{fig:opflow_cue_y}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\linewidth]{TechnicalBackground/optical_flow_x.png}
  \caption{Optical flow cue in x direction}
  \label{fig:opflow_cue_x}
\end{figure}
\begin{figure}[h!]  
  \includegraphics[width=1.0\linewidth]{TechnicalBackground/optical_flow_y.png}
  \caption{Optical flow in y direction}
  \label{fig:opflow_cue_y}
\end{figure}
$\mathbf{x} = (x, y)^{\tau}$ is the pixel position, $I_{t}$ is the temporal partial derivative and 
$\mathbf{u} = (\frac{\Delta x}{\Delta t}, \frac{\Delta y}{\Delta t})^{\tau}$ denotes the 2D velocity, which is the desired optical flow field. Apparently, the gradient constraint is an equation of two unknowns and cannot be solved without further constraint.

By assuming that the pixels in a neighborhood have the same 2D velocity, more gradient constraints of the nearby pixels can be added to constitute an over-determined system. The Lucas-Kanade method \cite{Lucas81aniterative} obtained a compromise solution by minimizing the sum of square errors
\begin{equation}
E(\mathbf{u}) = \sum_{\mathbf{X}_{i} \in \mathbf{S}}[\nabla I(\mathbf{x}_{i}, t) \cdot \mathbf{u} + I_{t}(\mathbf{x}_{i}, t))]^{2}
\end{equation}
where S denotes the set of all the pixels of a neighborhood centered around the target pixel. By setting the first derivatives of the sum of squared errors to zero, we have
\begin{equation}
\begin{aligned}
\frac{\partial E(\mathbf{u})}{\partial u_{x}} = \sum_{\mathbf{X}_{i} \in S}[u_{x} I_{x}^{2} + u_{y}I_{x}I_{y} + I_{x}I_{t}] = 0 \\
\frac{\partial E(\mathbf{u})}{\partial u_{y}} = \sum_{\mathbf{X}_{i} \in S}[u_{y} I_{y}^{2} + u_{x}I_{x}I_{y} + I_{y}I_{t}] = 0
\end{aligned}
\end{equation}
The least square estimate of optical flow field is
\begin{equation}
\hat{\mathbf{u}} = \begin{bmatrix} \displaystyle\sum_{\mathbf{x}_{i} \in S} I_{x}^{2} & \displaystyle\sum_{\mathbf{x}_{i} \in S} I_{x} I_{y}\\ \displaystyle\sum_{\mathbf{x}_{i} \in S} I_{x} I_{y}& \displaystyle\sum_{\mathbf{x}_{i} \in S} I_{y}^{2} \end{bmatrix}^{-1} \cdot \begin{bmatrix} -\displaystyle\sum_{\mathbf{x}_{i} \in S} I_{x} I_{t}\\ -\displaystyle\sum_{\mathbf{x}_{i} \in S} I_{y} I_{t} \end{bmatrix}
\end{equation}

One of the most commonly used algorithms of optical flow computation is proposed by Brox et al in \cite{Bro11a}. The author imposed penalizations on intensity error, gradient error, smoothness of optical flow field, descriptor matching task and descriptor matching error, resulting in a very sophisticated optimization problem.


\subsection{Pose}
Human pose, also known as human skeleton, is a high-level feature which represents the human body in a articulated system of multiple body joints. Pose can be either 2D (image coordinates) like shown in Figure \ref{subfig:cmu_skeleton} or 3D (with depth information) shown in Figure \ref{subfig:3d_skeleton} and can be hard coded where the joint positions of the human in the video are annotated manually. One popular method of annotation is the puppet method \cite{6751508}, which proposed a puppet to fit to human posture and recorded the corresponding joint locations of the puppet. While a convolutional pose machine is introduced by \cite{journals/corr/WeiRKS16} to directly estimate human pose from videos. As is shown in Figure \ref{fig:pose_cue}, the joint positions provide a meaningful representation of the human body with posture information. 

\begin{figure}[h!]
  \centering  
  \subfigure[2D pose skeleton\protect\footnotemark]
  {\label{subfig:cmu_skeleton}
  \includegraphics[width=0.2\textwidth]{TechnicalBackground/cmu_skeleton.png}}
  %\vspace{1em} % here you can insert horizontal or vertical space
  \subfigure[3D pose skeleton\protect\footnotemark]
  {\label{subfig:3d_skeleton}
  \includegraphics[width=0.6\textwidth]{TechnicalBackground/3d_skeleton.png}}  
  \caption{Example of 2D and 3D pose}
\end{figure}
\addtocounter{footnote}{-1} 
\footnotetext{\url{https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md}}\stepcounter{footnote} 
\footnotetext{\url{https://github.com/SrikanthVelpuri/tf-pose}}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\linewidth]{TechnicalBackground/pose.png}
  \caption{Pose cue}
  \label{fig:pose_cue}
\end{figure}

Compared to RGB and optical flows, pose is robust against variation of appearance and background clutters. In addition, 3D pose inherits the quality of viewpoint invariance, which facilitates the learning of cross-view features among pose representations in different viewpoints. Nowadays with the prevalence of depth camera and increasing sophistication of pose estimation algorithms, pose data are much easier to obtain, which paves the way for machine-learning-based, especially deep-learning-based algorithms for human pose estimation. 


\section{Deep Learning}
Deep learning is a subfield of machine learning which attempts to learn high-level abstractions in data by utilizing hierarchical architectures. It has been dramastically developing and widely applied in various traditional artificial intelligence domains, such as semantic parsing \cite{pmlr-v22-bordes12}, natural language processing \cite{DBLP:journals/corr/MikolovSCCD13}, computer vision \cite{DBLP:journals/corr/abs-1202-2745, krizhevsky2012} and so on. There are mainly three reasons for the booming of deep learning today: the dramatically increased chip processing abilities (e.g. GPU), the significantly lowered cost of computing hardware and the considerable advances in the machin learning algorithms \cite{deng_2014}.

Deep learning is basically built on the concept of "Artificial Neuron" \cite{McCulloch1943}, which was first proposed by McCulloch and Pitts in 1943. An artificial neuron is the basic computational unit of neural network similar to biological neuron \ref{subfig:neuron} in central nervous system of human brain. The mathematical model of neuron takes input, process the input and gives output in the same way as of biological neuron as shown in Figure \ref{subfig:neuron_model}. An artificial neuron that takes multiple inputs and gives one output is given by
\begin{equation}
y = f(\sum_{i}^{n} \omega_{i}x_{i} + b)
\end{equation}
Where $x_{i}$ are the inputs, $\omega_{i}$ and $b$ are the parameters of the neuron called weights and
bias respectively. $f$ denotes the activation function (as shown in Figure  \ref{fig:activation_function}) and $y$ is the output of neuron with i inputs. A single neuron can either be activated as "1" or deactivated as "0", thus behaving in a boolean pattern.

\begin{figure}[h!]
  \centering  
  \subfigure[Biological Neuron]
  {\label{subfig:neuron}
  \includegraphics[width=0.3\textwidth]{TechnicalBackground/neuron.png}}
  \hspace{4em} % here you can insert horizontal or vertical space
  \subfigure[Mathematical Model of Neuron]
  {\label{subfig:neuron_model}
  \includegraphics[width=0.3\textwidth]{TechnicalBackground/neuron_model.jpeg}}  
  \caption{Artificial Neuron \cite{cnnlecture2017}}
\end{figure}

\begin{figure}[h!]
  \centering  
  \subfigure[Sigmoid function]
  {\label{subfig:sigmoid}
  \includegraphics[width=0.3\textwidth]{TechnicalBackground/sigmoid.jpeg}}
  \hspace{4em} % here you can insert horizontal or vertical space
  \subfigure[Mathematical Model of Neuron]
  {\label{subfig:relu}
  \includegraphics[width=0.3\textwidth]{TechnicalBackground/relu.jpeg}}  
  \caption{Activation function \cite{cnnlecture2017}}
  \label{fig:activation_function}
\end{figure}


\subsection{Convolutional Neural Networks (CNNs)}
The Convolutional Neural Networks (CNN) is one of the most notable deep learning approaches where multiple layers are trained in an end-to-end manner \cite{lecun1998}. CNNs are composed of different layers, e.g. convolutional layers, pooling layer and fully connected layer. As images are composed of three channels, CNNs maintain the architecture in a more sensible way and the neurons are arranged in 3 dimensions: width, height, depth (here depth stands for another dimension of the activation volume not the depth of the network in terms of layers). For example, in CIFAR-10 dataset \cite{Krizhevsky09learningmultiple}, the images are of dimension $32 \times 32 \times 3$ (width,height,depth). CNN for such images is formulated in such a way that neurons in Convolutional layer are connected to only a small region of the preceding layer as shown in Figure \ref{subfig:cnn_architecture}. Additionally, the output layer of CNN has dimension of $1 \times 1 \times 10$ because there are 10 classes in the dataset. The scores of each class are arranged along the depth dimension. Figure \ref{subfig:cnn_car_pipeline} shows an example CNN pipeline for car classification.

\begin{figure}[h!]
  \centering  
  \subfigure[Architecture of Convolutional Neural Network. \cite{albel2017}]
  {\label{subfig:cnn_architecture}
  \includegraphics[width=0.65\textwidth]{TechnicalBackground/cnn_architecture.png}}
  %\hspace{1em} % here you can insert horizontal or vertical space
  \subfigure[Example pipeline of car classification \cite{cnnlecture2017}. The initial layer stores a raw RGB image and the last layer contains the class score.]
  {\label{subfig:cnn_car_pipeline}
  \includegraphics[width=0.65\textwidth]{TechnicalBackground/cnn_car_pipeline.jpeg}}  
  \caption{The pipeline of the general CNN architecture}
\end{figure}

As is mentioned above, a CNN consists of a series of different layers stacked in some order. In the following, three basic types of layer will be explained in detail.

\begin{description}[leftmargin=0in, labelindent=0pt]
\item[Convolutional Layer]  {This is the core building block of CNN, which is also responsible for decreasing the computational time as compared to regular network layers. The neurons in this layer are connected to a small 3D input volume (receptive field) \cite{cnnlecture2017}. This ensures the local connectivity of neurons to the previous layer. The neurons in the receptive field are connected to the output volume of neurons, as shown in Figure \ref{fig:cnn_layer_input}. The learn-able weights of these connections form a matrix which is called kernel or filter. These filters slide over the whole input layer and forms a 3D activation volume in the output layer. 3D activation volume is formed by dot product between the entries of the filter and the input, adding a bias and applying the activation function.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.3\linewidth]{TechnicalBackground/depthcol.jpeg}
  \caption{An example input volume in red (e.g. a $32 \times 32 \times 3$ CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). \cite{cnnlecture2017}}
  \label{fig:cnn_layer_input}
\end{figure}

The size of output layer is controlled by hyper-parameters namely depth (no. of filters), stride and zero padding. The filters are responsible for capturing abstract representations and features from input data. If the input layer is a 3-channel RGB image, then the filters in early convolutional layers may activate upon seeing edges, colour blobs in the image \cite{cnnlecture2017}. 

With a stride, the spatial dimension of the filter can be changed. In Figure \ref{fig:conv_layer}, one channel of input image with dimension $5 \times 5$ is shown. A receptive field of $3 \times 3$ (yellow square) convolving over an image of $5 \times 5$ (green square) produces an output of $3 \times 3$ (pink square). Weights of the filter are shown as red digits in the yellow square. Here, the stride of 1 is used, which means the filter slides over one pixel. Because of this stride, the output dimension is changed to $3 \times 3$. With stride, the size of output volume gets reduced as compared to input volume. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\linewidth]{TechnicalBackground/conv_layer.png}
  \caption{An image of $5 \times 5$ (green) convolved by a receptive field of $3 \times 3$ (yellow) with stride of 1 to produce output filter of size $3 \times 3$ (Pink). \cite{kandeng2013}}
  \label{fig:conv_layer}
\end{figure}

However, to keep the output volume equal to the input volume, zero padding around input volume is used. The size of output volume in which the neurons would fit is calculated by 
\begin{equation}
O = \frac{W - F + 2P}{S} + 1
\end{equation}
where $O$ is the output size, $W$ is the input dimension, $F$ is the filter size, $P$ is the zero
padding and $S$ is the stride.
} 

\item[Pooling Layer] {It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size $2 \times 2$ applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75\% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little $2 \times 2$ region in some depth slice). The depth dimension remains unchanged. 

\begin{figure}[h!]
  \centering  
  \subfigure[]
  {\label{subfig:pool}
  \includegraphics[width=0.3\textwidth]{TechnicalBackground/pool.jpeg}}
  %\hspace{1em} % here you can insert horizontal or vertical space
  \subfigure[]
  {\label{subfig:maxpool}
  \includegraphics[width=0.5\textwidth]{TechnicalBackground/maxpool.jpeg}}  
  \caption{\textbf{Left}: In this example, the input volume of size $[224 \times 224 \times 64]$ is pooled with filter size 2, stride 2 into output volume of size $[112 \times 112 \times 64]$. Notice that the volume depth is preserved. \textbf{Right}: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little $2 \times 2$ square). \cite{cnnlecture2017}}
  \label{fig:pooling_layer}
\end{figure}


}

\item[Full Connected Layer] {Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in Figure \ref{fig:full_conn_layer}. Their activations can hence be computed with a matrix multiplication followed by a bias offset. It enables us to feed forward the neural network into a vector with pre-defined length. We could either feed forward the vector into certain number categories for image classification \cite{krizhevsky2012} or take it as a feature vector for follow-up processing \cite{DBLP:journals/corr/GirshickDDM13}.  

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\linewidth]{TechnicalBackground/full_conn_layer.jpeg}
  \caption{A regular neural network \cite{cnnlecture2017}}
  \label{fig:full_conn_layer}
\end{figure}

}
\end{description}


\section{Robot Operating System (ROS)}
\label{subsec:ros}
ROS is a Linux-based, open-source, middleware framework for modular use in robot applications. ROS, originally designed by Willow Garage and currently maintained by the Open Source Robotics Foundation, is a powerful tool because it utilizes object-oriented programming, a method of programming organized around data rather than procedures in its interaction with data and communication within a modular system \cite{rosintro}. ROS is divided into three conceptual levels: the filesystem level, the computation graph level and the community level.

\subsection{Filesystem Level}
The filesystem level is the organization of the ROS framework on a machine, as shown in Figure \ref{fig:ros_filesystem}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\linewidth]{TechnicalBackground/ros_filesystem.png}
  \caption{ROS filesystem level}
  \label{fig:ros_filesystem}
\end{figure}

Here are the explanations of each block in the file system:
\begin{description}[leftmargin=0in, labelindent=0pt]
\item[Package]: {The ROS packages are the most basic unit of the ROS software. It contains the ROS runtime process (nodes), libraries, configuration files, and so on, which are organized together as a single unit. Packages are the atomic build item and release item in the ROS software.

Figure \ref{fig:ros_pkg_struct} shows a typical structure of a ROS package.
%and screenshot \ref{fig:ros_pkg_file_structure} presents a demo package layout in ros workspace.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.55\linewidth]{TechnicalBackground/ros_pkg_structure.png}
  \caption{Structure of a typical ROS package}
  \label{fig:ros_pkg_struct}
\end{figure}

%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=0.5\linewidth]{TechnicalBackground/ros_pkg_demo.png}
%  \caption{A screenshot of demo package file structure}
%  \label{fig:ros_pkg_file_structure}
%\end{figure}

We can discuss the use of each folder as follows:
\begin{itemize}%[leftmargin=0.18in, labelindent=0pt]
\item[•] \texttt{action}\footnote{\url{http://wiki.ros.org/actionlib}}: Folder containing Action types, which define the Goal, Feedback and Result of the action
\item[•] \texttt{config}: All configuration files that are used in this ROS package are kept in this folder. This folder is created by the user and is a common practice to name the folder config to keep the configuration files in it.
\item[•] \texttt{include/package\_name}: C++ include headers
\item[•] \texttt{launch}: This folder keeps the launch files that are used to launch one or more ROS nodes.
\item[•] \texttt{msg}: Folder containing Message types, which define the structure of data for messages sent via ROS
\item[•] \texttt{src}: Source files, including Python source that are exported to other packages and C++ source file
\item[•] \texttt{srv}: Folder containing Service types,  which define the request and response data structures for services
\item[•] \texttt{scripts}: executable scripts, mostly python nodes
\item[•] \texttt{CMakeLists.txt}: CMake build file
\item[•] \texttt{package.xml}: Package manifests, providing package metadata, such as the name, author, version, description, license, dependencies, compilation flags and so on. 
%Figure \ref{fig:ros_pkg_manifest} shows a screenshot of the demo package.
\end{itemize}

%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=0.6\linewidth]{TechnicalBackground/ros_pkg_manifest.png}
%  \caption{A screenshot of demo package manifest}
%  \label{fig:ros_pkg_manifest}
%\end{figure}

}

\item[Message](\texttt{.msg}): {The ROS messages are a type of information that is sent among ROS nodes. We can define a custom message inside the msg folder inside a package. The extension of the message file is \texttt{.msg}. The message definition can consist of two types: \texttt{fields} and \texttt{constants} . The field is split into field types and field name. Field types is the data type \footnote{\url{http://wiki.ros.org/msg}} of the transmitting message and field name is the name of it. The constants define a constant value in the message file. An example of a \texttt{msg} file is as follows, shown in Tabel \ref{table:msg_example}:
\begin{table}[h!]
\centering
\begin{tabular}{|lll|ll}
\cline{1-3}
\texttt{int32}   &  & \texttt{id}         &  &  \\
\texttt{float32} &  & \texttt{vel}        &  &  \\
\texttt{string}  &  & \texttt{name="car"} &  & \\ \cline{1-3}
\end{tabular}
\caption{An example of message definition}
\label{table:msg_example}
\end{table}
} 

\item[Services](\texttt{.srv}): {The ROS service is a type of request/response communication between ROS nodes. One node will send a request and wait until it gets a response from the other. The request/response communication is also using the ROS message description, which can be defined inside the srv folder inside the package. An example service description format is as follows in Tabel \ref{table:srv_example}:

\begin{table}[h!]
\centering
\begin{tabular}{|l|}
\hline
\begin{tabular}[c]{@{}l@{}}\texttt{\#Request message type}\\ \texttt{string \hspace{1em} req\_str}\end{tabular}  \\
\texttt{----}                                                                           \\
\begin{tabular}[c]{@{}l@{}}\texttt{\#Response message type}\\ \texttt{string \hspace{1em} res\_str}\end{tabular} \\ \hline
\end{tabular}
\caption{An example of service definition}
\label{table:srv_example}
\end{table}

The first section is the message type of request that is separated by \texttt{----} and in the next section is the message type of response. In these examples, both Request and Response are strings.
}

\item[Meta packages]: {Meta packages are specialized packages in ROS that only contain one file, that is, a package.xml file. It doesn't contain folders and files similar to a normal package. One of the examples of a meta package is the ROS navigation stack \footnote{\url{http://wiki.ros.org/navigation}}. 
Meta packages simply group a set of multiple packages as a single logical package. 
%In the package.xml file, the meta package contains an export tag, as shown here:

%\begin{table}[H]
%\begin{tabular}{l}
%\hspace{1em}\texttt{\textless{}export\textgreater{}}       \\
%\hspace{3em}\texttt{\textless{}metapackage/\textgreater{}} \\
%\hspace{1em}\texttt{\textless{}/export\textgreater{}}     
%\end{tabular}
%\end{table}
%
%Also, in meta packages, there are no build dependencies for catkin, only \texttt{<exec\_depend>} dependencies are needed, which are the packages grouped in the meta package as follows:
%
%\begin{table}[H]
%\begin{tabular}{l}
%\hspace{1em}\texttt{\textless{}exec\_depend\textgreater{}}
%\texttt{subpackage\_1}
%\texttt{\textless{}/exec\_depend\textgreater{}}  \\
%\hspace{1em}\texttt{\textless{}exec\_depend\textgreater{}}
%\texttt{subpackage\_2}
%\texttt{\textless{}/exec\_depend\textgreater{}}
%\end{tabular}
%\end{table}
}  

\item[Repositories]: {Most of the ROS packages are maintained using a Version Control System (VCS) such as Git \footnote{\url{https://github.com/}} and SVN \footnote{\url{https://subversion.apache.org/}}. The collection of packages that share a common VCS can be called repositories. The package in the repositories can be released using a catkin \footnote{\url{https://catkin-tools.readthedocs.io/en/latest/}} release automation tool called \texttt{bloom}\footnote{\url{http://wiki.ros.org/bloom}}.
}
\end{description}


\subsection{Computation Graph Level}
\label{ros:computaion_graph}
The computation graph level is where ROS processes data within a peer-to-peer network. The basic elements of ROS’s computation graph level are ROS \textbf{Nodes}, \textbf{Master}, \textbf{Parameter Server}, \textbf{Messages}, \textbf{Topics}, \textbf{Services} and \textbf{Bags}\footnote{\url{http://wiki.ros.org/rosbag}}. Figure \ref{fig:ros_graph} provides an overview of the ROS computation graph. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\linewidth]{TechnicalBackground/ros_graph.png}
  \caption{Structure of the ROS Graph layer}
  \label{fig:ros_graph}
\end{figure}

\begin{description}[leftmargin=0in, labelindent=0pt]
\item[Master] {The ROS Master provides naming and registration services as well as \texttt{Parameter Server} to the rest of the nodes in the ROS system. It tracks publishers and subscribers to topics as well as services. The role of the Master is to enable individual ROS nodes to locate one another. Once these nodes have located each other, they communicate with each other peer-to-peer \cite{rosmaster}. In a distributed system, the ROS master should be running on one computer, and remote nodes launched on other machines should register to that master via an environment variable \texttt{ROS\_MASTER\_URL}\footnote{\url{http://wiki.ros.org/ROS/Tutorials/MultipleMachines}}, which contains the IP and port of ROS Master, only then can the communication between nodes take place. 
}

\item[Nodes] {A node is a process that performs computation and serves as the atomic unit in the computation graph. Since ROS is designed to be modular at a fine-grained scale, an application or pipeline usually comprises multiple nodes. For instance, in a robot control system, one node controls a laser range-finder, one node controls the wheel motors, one node performs localization, one node performs path planning, one Node provides a graphical view of the system and so on. One ROS package could also hold multiple nodes, which are written either in C++ (using roscpp\footnote{\url{http://wiki.ros.org/roscpp}}) or Python (using rospy\footnote{\url{http://wiki.ros.org/rospy}}). Moreover, one single node can provide \texttt{Publishers}, \texttt{Subscribers} as well as \texttt{Services}, which will be discussed in the following. 

\begin{itemize}%[leftmargin=0.18in, labelindent=0pt]
\item[•] \texttt{Publisher and Subscriber}{ \\ 
ROS nodes communicate with each other by publishing messages to topics, which will be subscribed by other nodes. As is shown in Figure \ref{fig:ros_pub_sub}, the first step, Node 1 (publisher) and Node 2 (subscriber) commit registration to ROS Master. The publisher set in Node 1 publishes messages of certain message type to some topic,	then other nodes, which have subscribed to the same topic, can receive the messages at a pre-defined rate. Based on this publish/subscribe mechanism, ROS nodes share a high degree of flexibility and can be easily replaced, as long as the alternative nodes publish messages of the same type to the same topic or subscribe to the same topic and process the same type of message after subscription. One example in computer vision is, the publishers send image messages (\texttt{sensor\_msgs/Image}) to topic \texttt{image\_topic/image\_raw}, and the subscribers take the image messages and process it for different purposes, while the images can be either captured online or read from local files.      

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\linewidth]{TechnicalBackground/ros_pub_sub.png}
  \caption{ROS publisher and subscriber communication \cite{roseth2018}}
  \label{fig:ros_pub_sub}
\end{figure}
}

\item[•] \texttt{Nodelet} {\\
Nodelets are designed to provide a way to run multiple algorithms on a single machine, in a single process, without incurring copy costs when passing messages intraprocess. roscpp has optimizations to do zero copy pointer passing between publish and subscribe calls within the same node. To do this nodelets allow dynamic loading of classes into the same node, however they provide simple separate namespaces such that the nodelet acts like a seperate node, despite being in the same process \cite{rosnodelet}. ROS nodelet is useful when multiple processes need to use messages which contain large amounts of data (e.g. images or point clouds), as packaging the message, sending it and then unpackaging it can take a bit of time. Nodelet makes the resource (data) shared among nodes in the same process (\texttt{NodeletManager}\footnote{\url{http://wiki.ros.org/nodelet/Tutorials/Running\%20a\%20nodelet}}). Nodelets don't make the processes quicker, but it is a quicker way to get information (messages) from one node to another node, avoiding copying and network traffic.
} 


\item[•] \texttt{Service} {\\
The publish/subscribe model is a very flexible communication paradigm, but its many-to-many one-way transport is not appropriate for RPC request/reply interactions, which are often required in a distributed system. Request/reply is done via a service, which is defined by a pair of messages: one for the request and one for the reply. A server ROS node offers a service under a string name, and a client calls the service by sending the request message and awaiting the reply \cite{rosservice}. Figure \ref{fig:ros_service} presents how ROS service functions.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{TechnicalBackground/ros_service.png}
  \caption{ROS service \cite{roseth2018}}
  \label{fig:ros_service}
\end{figure} 
} 
\end{itemize}
}

\item[Parameter Server] {
ROS parameter server is a shared, multi-variate dictionary that is accessible via network APIs. 
As a part of ROS Master, parameter server allows the values of parameters to be stored and retrieved by nodes at runtime \cite{rosparamserver}. This is useful to configure the nodes before launched. The parameters can be defined in \texttt{launch files} or seprated \texttt{YAML files} in \texttt{config} folder within the package.

\begin{itemize}%[leftmargin=0.18in, labelindent=0pt]
\item[•] \texttt{Launch file}{\\
Many ROS packages come with "launch files", which can be run with:

\texttt{\$ roslaunch package\_name file.launch}

These launch files usually bring up a set of nodes for the package that provide some aggregate functionality. As mentioned above, in launch file, different values for different parameters can be configured. An example is presented below, which launches three nodes: \texttt{cv\_camera\_node}, \texttt{person\_detector}, \texttt{person\_tracker} and sets three parameters in \texttt{cv\_camera\_node} using \texttt{\textless{}param name="" value=""/\textgreater} paradigm.

\texttt{\textless{}launch\textgreater}\\   
\hspace*{2em}\texttt{\textless{}arg name="video\_file" value="demo.mp4"/\textgreater}\\   
\hspace*{2em}\texttt{\textless{}!--- video stream input ---\textgreater}\\   
\hspace*{2em}\texttt{\textless{}node pkg="cv\_camera" name="cv\_camera" type="cv\_camera\_node" output="screen"\textgreater}\\     
\hspace*{4em}\texttt{\textless{}param name="file" value="\$(arg video\_file)"/\textgreater}\\     
\hspace*{4em}\texttt{\textless{}param name="rate" value="0.5"/\textgreater}\\     
\hspace*{4em}\texttt{\textless{}param name="wait\_for\_subscriber" value="true"/\textgreater}\\   
\hspace*{2em}\texttt{\textless{}/node\textgreater}\\ 
\hspace*{2em}\texttt{\textless{}!--- person detection node ---\textgreater}\\   
\hspace*{2em}\texttt{\textless{}node pkg="person\_detector" name="person\_detector" type="person\_detector" output="screen"/\textgreater}\\ 
\hspace*{2em}\texttt{\textless{}!--- person tracking node ---\textgreater}\\   
\hspace*{2em}\texttt{\textless{}node pkg="person\_tracker" name="person\_tracker" type="person\_tracker" output="screen"/\textgreater}\\
\texttt{\textless{}/launch\textgreater{}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  }

\vspace{1em}
\item[•] \texttt{YAML file}{\\
Another way to configure node parameters is to use \texttt{YAML} file as follows:\\ 
\texttt{config.yaml} \\
\texttt{------------------} \\
\texttt{file: demo.mp4} \\
\texttt{rate: 0.5} \\
\texttt{wait\_for\_subscriber: true}
} 
\end{itemize}
}

\item[Rqt User Interface]{
Rqt is a software framework of ROS that implements the various GUI tools in the form of plugins. Here we would like to mention two of them: \texttt{rqt\_graph} and \texttt{rqt\_console}. 

\begin{itemize}
\item[•] \texttt{rqt\_graph} \\
\texttt{rqt\_graph} provides a GUI plugin for visualizing the ROS computation graph, including all relevant nodes and topics. Figure \ref{fig:rqt_graph} shows an example of \texttt{rqt\_graph}: node \texttt{cv\_camera} publishes image messages to topic \texttt{/cv\_camera/image\_raw}, and \texttt{image\_view} node subscribes to the same topic. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{TechnicalBackground/rqt_graph.png}
  \caption{An example of rqt\_graph}
  \label{fig:rqt_graph}
\end{figure} 

\item[•] \texttt{rqt\_console} \\
Rather than computation graph, \texttt{rqt\_console} provides a GUI plugin for displaying and filtering ROS messages, like displayed in Figure \ref{fig:rqt_console}. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{TechnicalBackground/rqt_console.png}
  \caption{An example of rqt\_consoles}
  \label{fig:rqt_console}
\end{figure}
\end{itemize}

\texttt{rqt\_graph} and \texttt{rqt\_console} are very useful tools for development, testing as well as debugging nodes, therefore we use it a lot during the development of the pipeline and later we will take advantage of them to showcase the pipeline architecture and its APIs.
}
\end{description}

\subsection{Community Level}
The ROS Community Level consists of ROS distributions, repositories, ROS Wiki, and ROS Answers, which encourages the communication and resource-sharing among robotics community across the world \cite{rosconcept}.

\let\cleardoublepage\clearpage

