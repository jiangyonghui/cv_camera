\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Mohan:2001:EOD:377051.377062}
\citation{viola2003}
\citation{viola2001}
\citation{papag1998}
\citation{viola2003}
\citation{Mohan:2001:EOD:377051.377062}
\citation{Elgammal2000}
\citation{Siden2003}
\citation{stau1999}
\citation{toyama1999}
\citation{Dalal2005}
\citation{Cortes1995}
\citation{wang2009}
\citation{ahonen2006}
\citation{dala2006}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{18}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:related_work}{{3}{18}{Related Work}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Person Detection}{18}{section.3.1}}
\newlabel{related_work:pd}{{3.1}{18}{Person Detection}{section.3.1}{}}
\citation{krizhevsky2012}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{DBLP:journals/corr/Girshick15}
\citation{DBLP:journals/corr/RenHG015}
\citation{DBLP:journals/corr/LiuAESR15}
\citation{DBLP:journals/corr/RedmonDGF15}
\citation{DBLP:journals/corr/LiuAESR15}
\citation{DBLP:journals/corr/LiuAESR15}
\citation{DBLP:journals/corr/LiuAESR15}
\citation{DBLP:journals/corr/RedmonDGF15}
\citation{DBLP:journals/corr/RedmonDGF15}
\citation{broida1986}
\citation{shi1994}
\citation{shi1994}
\citation{coman2003}
\citation{avidan2007}
\citation{collins2005}
\citation{Grabner2006}
\citation{Babenko2009}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.1}{\ignorespaces \textbf  {R-CNN object detection system overview.} (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class- specific linear SVMs. \cite  {DBLP:journals/corr/GirshickDDM13}}}{19}{figure.3.1}}
\newlabel{fig:r_cnn}{{\relax 3.1}{19}{\textbf {R-CNN object detection system overview.} (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class- specific linear SVMs. \cite {DBLP:journals/corr/GirshickDDM13}}{figure.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Person Tracking}{19}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Single Object Tracking}{19}{subsection.3.2.1}}
\citation{kalal2010}
\citation{dinh2011}
\citation{hare2016}
\citation{Welch1995}
\citation{vasuhi2015}
\citation{Katsarakis2006}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.2}{\ignorespaces \textbf  {SSD framework.} (a) SSD only needs an input image and ground truth boxes for each object during training. In a convolutional fashion, a small set (e.g. 4) of default boxes of different aspect ratios at each location in several feature maps with different scales is to be evaluated (e.g. $8 \times 8$ and $4 \times 4$ in (b) and (c)). For each default box, both the shape offsets and the confidences for all object categories (($c_{1}, c_{2}, \cdots  , c_{p}$)) are predicted. \cite  {DBLP:journals/corr/LiuAESR15}}}{20}{figure.3.2}}
\newlabel{fig:ssd}{{\relax 3.2}{20}{\textbf {SSD framework.} (a) SSD only needs an input image and ground truth boxes for each object during training. In a convolutional fashion, a small set (e.g. 4) of default boxes of different aspect ratios at each location in several feature maps with different scales is to be evaluated (e.g. $8 \times 8$ and $4 \times 4$ in (b) and (c)). For each default box, both the shape offsets and the confidences for all object categories (($c_{1}, c_{2}, \cdots , c_{p}$)) are predicted. \cite {DBLP:journals/corr/LiuAESR15}}{figure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.3}{\ignorespaces \textbf  {The YOLO model.} The system models detection as a regression problem. It divides the image into an $S \times S$ grid and for each grid cell predicts $B$ boundingboxes, confidence for those boxes, and $C$ class probabilities. These predictions are encoded as an $S \times S \times (B * 5 + C)$ tensors. \cite  {DBLP:journals/corr/RedmonDGF15}}}{20}{figure.3.3}}
\newlabel{fig:yolo}{{\relax 3.3}{20}{\textbf {The YOLO model.} The system models detection as a regression problem. It divides the image into an $S \times S$ grid and for each grid cell predicts $B$ boundingboxes, confidence for those boxes, and $C$ class probabilities. These predictions are encoded as an $S \times S \times (B * 5 + C)$ tensors. \cite {DBLP:journals/corr/RedmonDGF15}}{figure.3.3}{}}
\citation{blackman2004}
\citation{zhang2008}
\citation{brendel2011}
\citation{benfold2011}
\citation{Kuo2010}
\citation{Zhang2010bow}
\citation{Cortes1995}
\citation{Dalal2005}
\citation{Laptev2008}
\citation{Lowe2004}
\citation{Bay2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Multiple Object Tracking}{21}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Action Recognition}{21}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Non-Pose-based Action Recognition}{21}{subsection.3.3.1}}
\newlabel{subsec:non_pose_based_ar}{{3.3.1}{21}{Non-Pose-based Action Recognition}{subsection.3.3.1}{}}
\citation{wang2013}
\citation{wang2013}
\citation{wang2013}
\citation{dala2006}
\citation{wang2011}
\citation{Fischler1981}
\citation{Reddy2013}
\citation{kuehne2011}
\citation{wang2013}
\citation{DBLP:journals/corr/SimonyanZ14a}
\citation{DBLP:journals/corr/SimonyanZ14}
\citation{DBLP:journals/corr/FeichtenhoferPZ16}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.1}Handcrafted-Feature-based Approaches}{22}{subsubsection.3.3.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.4}{\ignorespaces Left: Dense sampling of feature points in different spatial scales. Middle: Tracking in an optical flow field. Right: descriptors along the trajectory. \cite  {wang2013}}}{22}{figure.3.4}}
\newlabel{fig:idt}{{\relax 3.4}{22}{Left: Dense sampling of feature points in different spatial scales. Middle: Tracking in an optical flow field. Right: descriptors along the trajectory. \cite {wang2013}}{figure.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.2}Deep-Learning-based Approaches}{22}{subsubsection.3.3.1.2}}
\citation{DBLP:journals/corr/SimonyanZ14}
\citation{DBLP:journals/corr/SimonyanZ14}
\citation{DBLP:journals/corr/WangXW0LTG16}
\citation{DBLP:journals/corr/WangXW0LTG16}
\citation{DBLP:journals/corr/WangXW0LTG16}
\citation{DBLP:journals/corr/SimonyanZ14a}
\citation{kuehne2011}
\citation{Soomro2012}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.5}{\ignorespaces Two-stream structure for video classification. \cite  {DBLP:journals/corr/SimonyanZ14}}}{23}{figure.3.5}}
\newlabel{fig:two_stream_convnet}{{\relax 3.5}{23}{Two-stream structure for video classification. \cite {DBLP:journals/corr/SimonyanZ14}}{figure.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.6}{\ignorespaces Complete pipeline of Temporal Segment Networks. \cite  {DBLP:journals/corr/WangXW0LTG16}}}{23}{figure.3.6}}
\newlabel{fig:tsn}{{\relax 3.6}{23}{Complete pipeline of Temporal Segment Networks. \cite {DBLP:journals/corr/WangXW0LTG16}}{figure.3.6}{}}
\citation{DBLP:journals/corr/DonahueHGRVSD14}
\citation{Hochreiter1997}
\citation{DBLP:journals/corr/NgHVVMT15}
\citation{cao2017realtime}
\citation{cao2017realtime}
\citation{cao2017realtime}
\citation{cao2017realtime}
\citation{cao2017realtime}
\citation{Vedaldi2015}
\citation{DBLP:journals/corr/LinMBHPRDZ14}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Pose-based Action Recognition}{24}{subsection.3.3.2}}
\newlabel{subsec:pose_based_ar}{{3.3.2}{24}{Pose-based Action Recognition}{subsection.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.7}{\ignorespaces Multi stage two branch architecture for pose estimation by part affinity field. \cite  {cao2017realtime}}}{24}{figure.3.7}}
\newlabel{fig:openpose_arch}{{\relax 3.7}{24}{Multi stage two branch architecture for pose estimation by part affinity field. \cite {cao2017realtime}}{figure.3.7}{}}
\citation{DBLP:journals/corr/ZhouH0XW17}
\citation{DBLP:journals/corr/ZhouH0XW17}
\citation{DBLP:journals/corr/ZhouH0XW17}
\citation{DBLP:journals/corr/ZhouH0XW17}
\citation{DBLP:journals/corr/ZhouH0XW17}
\citation{DBLP:conf/eccv/NewellYD16}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.8}{\ignorespaces Overall pipeline of pose estimation by part affinity field technique. \cite  {cao2017realtime}}}{25}{figure.3.8}}
\newlabel{fig:openpose}{{\relax 3.8}{25}{Overall pipeline of pose estimation by part affinity field technique. \cite {cao2017realtime}}{figure.3.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.9}{\ignorespaces The stacked hourglass network computes the 2D heat maps (2D joints) based on the image features extracted by Conv layers and the depth regression module computes the depth value based on summation of 2D heat maps and image features extracted in the beginning. \cite  {DBLP:journals/corr/ZhouH0XW17}}}{25}{figure.3.9}}
\newlabel{fig:3d_pose_model}{{\relax 3.9}{25}{The stacked hourglass network computes the 2D heat maps (2D joints) based on the image features extracted by Conv layers and the depth regression module computes the depth value based on summation of 2D heat maps and image features extracted in the beginning. \cite {DBLP:journals/corr/ZhouH0XW17}}{figure.3.9}{}}
\citation{DBLP:journals/corr/Girshick15}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{DBLP:journals/corr/RenHG015}
\citation{lin2017}
\citation{lin2017}
\citation{lin2017}
\citation{DBLP:journals/corr/XiongZWLT17}
\citation{DBLP:journals/corr/XiongZWLT17}
\citation{DBLP:journals/corr/XiongZWLT17}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.10}{\ignorespaces A schematic illustration of our method: transferring 3D annotation from indoor images to in-the-wild images. Top (Training): Both indoor images with 3D annotation (Right) and in-the-wild images with 2D annotation (Left) are used to train the deep neural network. Bottom (Testing): The learned network can predict the 3D pose of the human in in-the-wild images. \cite  {DBLP:journals/corr/ZhouH0XW17}}}{26}{figure.3.10}}
\newlabel{fig:3d_pose_arch}{{\relax 3.10}{26}{A schematic illustration of our method: transferring 3D annotation from indoor images to in-the-wild images. Top (Training): Both indoor images with 3D annotation (Right) and in-the-wild images with 2D annotation (Left) are used to train the deep neural network. Bottom (Testing): The learned network can predict the 3D pose of the human in in-the-wild images. \cite {DBLP:journals/corr/ZhouH0XW17}}{figure.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Temporal Action Detection}{26}{section.3.4}}
\newlabel{subsec:tem_action_det}{{3.4}{26}{Temporal Action Detection}{section.3.4}{}}
\citation{DBLP:journals/corr/XiongZWLT17}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.11}{\ignorespaces \textbf  {Overview of SSAD framework}. Given an untrimmed long video, (1) Snippet-level Action Score features sequence with multiple action classifiers are extracted; (2) SSAD network takes feature sequence as input and directly predicts multiple scales action instances without proposal generation step. \cite  {lin2017}}}{27}{figure.3.11}}
\newlabel{fig:ssad}{{\relax 3.11}{27}{\textbf {Overview of SSAD framework}. Given an untrimmed long video, (1) Snippet-level Action Score features sequence with multiple action classifiers are extracted; (2) SSAD network takes feature sequence as input and directly predicts multiple scales action instances without proposal generation step. \cite {lin2017}}{figure.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.12}{\ignorespaces An illustration of the temporal actionness grouping algorithm. \cite  {DBLP:journals/corr/XiongZWLT17}}}{27}{figure.3.12}}
\newlabel{fig:action_grouping}{{\relax 3.12}{27}{An illustration of the temporal actionness grouping algorithm. \cite {DBLP:journals/corr/XiongZWLT17}}{figure.3.12}{}}
\@setckpt{Main/3_Related_Work}{
\setcounter{page}{28}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{12}
\setcounter{table}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{21}
\setcounter{bookmark@seq@number}{30}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{1}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{1}
\setcounter{Changes@AuthorCount}{1}
\setcounter{Changes@Author}{0}
\setcounter{Changes@AddCount}{0}
\setcounter{Changes@DeleteCount}{0}
\setcounter{Changes@ReplaceCount}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{float@type}{8}
\setcounter{lstnumber}{1}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{section@level}{1}
\setcounter{lstlisting}{0}
}
